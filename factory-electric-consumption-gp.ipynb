{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-28T21:02:25.321990Z","iopub.execute_input":"2024-12-28T21:02:25.322373Z","iopub.status.idle":"2024-12-28T21:02:25.332037Z","shell.execute_reply.started":"2024-12-28T21:02:25.322342Z","shell.execute_reply":"2024-12-28T21:02:25.330892Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/prediction-of-factory-electric-consumption/submission.csv\n/kaggle/input/prediction-of-factory-electric-consumption/train_df.csv\n/kaggle/input/prediction-of-factory-electric-consumption/test_df.csv\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"!pip install gpytorch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T21:02:25.333356Z","iopub.execute_input":"2024-12-28T21:02:25.333722Z","iopub.status.idle":"2024-12-28T21:02:29.076456Z","shell.execute_reply.started":"2024-12-28T21:02:25.333685Z","shell.execute_reply":"2024-12-28T21:02:29.075285Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: gpytorch in /usr/local/lib/python3.10/dist-packages (1.13)\nRequirement already satisfied: jaxtyping==0.2.19 in /usr/local/lib/python3.10/dist-packages (from gpytorch) (0.2.19)\nRequirement already satisfied: mpmath<=1.3,>=0.19 in /usr/local/lib/python3.10/dist-packages (from gpytorch) (1.3.0)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from gpytorch) (1.2.2)\nRequirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from gpytorch) (1.13.1)\nRequirement already satisfied: linear-operator>=0.5.3 in /usr/local/lib/python3.10/dist-packages (from gpytorch) (0.5.3)\nRequirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from jaxtyping==0.2.19->gpytorch) (1.26.4)\nRequirement already satisfied: typeguard>=2.13.3 in /usr/local/lib/python3.10/dist-packages (from jaxtyping==0.2.19->gpytorch) (4.3.0)\nRequirement already satisfied: typing-extensions>=3.7.4.1 in /usr/local/lib/python3.10/dist-packages (from jaxtyping==0.2.19->gpytorch) (4.12.2)\nRequirement already satisfied: torch>=1.11 in /usr/local/lib/python3.10/dist-packages (from linear-operator>=0.5.3->gpytorch) (2.4.1+cu121)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->gpytorch) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->gpytorch) (3.5.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (3.16.1)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (3.3)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.11->linear-operator>=0.5.3->gpytorch) (2024.6.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11->linear-operator>=0.5.3->gpytorch) (2.1.5)\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/prediction-of-factory-electric-consumption/train_df.csv')\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T21:02:29.078325Z","iopub.execute_input":"2024-12-28T21:02:29.078613Z","iopub.status.idle":"2024-12-28T21:02:29.124913Z","shell.execute_reply.started":"2024-12-28T21:02:29.078589Z","shell.execute_reply":"2024-12-28T21:02:29.124049Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"                      Date  Electric_Consumption  Factor_A   Factor_B  \\\n0  2023-01-01T00:00:00.000              0.000000  1.242130  28.419739   \n1  2023-01-01T01:00:00.000              0.000000  1.861285  29.840759   \n2  2023-01-01T02:00:00.000              0.000000  4.212674  32.778036   \n3  2023-01-01T03:00:00.000              0.000000  4.025251  32.624700   \n4  2023-01-01T04:00:00.000             -0.000267  3.122659  31.931245   \n\n    Factor_C   Factor_D  Factor_E  Factor_F  \n0  13.720397  79.840600       0.0  2.386157  \n1  12.537668  86.424903       0.0  1.473256  \n2   9.408667  72.082793       0.0  1.583711  \n3   9.035601  73.825705       0.0  1.706656  \n4   9.235502  66.823956       0.0  0.987048  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Date</th>\n      <th>Electric_Consumption</th>\n      <th>Factor_A</th>\n      <th>Factor_B</th>\n      <th>Factor_C</th>\n      <th>Factor_D</th>\n      <th>Factor_E</th>\n      <th>Factor_F</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2023-01-01T00:00:00.000</td>\n      <td>0.000000</td>\n      <td>1.242130</td>\n      <td>28.419739</td>\n      <td>13.720397</td>\n      <td>79.840600</td>\n      <td>0.0</td>\n      <td>2.386157</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2023-01-01T01:00:00.000</td>\n      <td>0.000000</td>\n      <td>1.861285</td>\n      <td>29.840759</td>\n      <td>12.537668</td>\n      <td>86.424903</td>\n      <td>0.0</td>\n      <td>1.473256</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2023-01-01T02:00:00.000</td>\n      <td>0.000000</td>\n      <td>4.212674</td>\n      <td>32.778036</td>\n      <td>9.408667</td>\n      <td>72.082793</td>\n      <td>0.0</td>\n      <td>1.583711</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2023-01-01T03:00:00.000</td>\n      <td>0.000000</td>\n      <td>4.025251</td>\n      <td>32.624700</td>\n      <td>9.035601</td>\n      <td>73.825705</td>\n      <td>0.0</td>\n      <td>1.706656</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2023-01-01T04:00:00.000</td>\n      <td>-0.000267</td>\n      <td>3.122659</td>\n      <td>31.931245</td>\n      <td>9.235502</td>\n      <td>66.823956</td>\n      <td>0.0</td>\n      <td>0.987048</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"import torch\nimport gpytorch\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nfrom datetime import datetime\n\nclass VariationalGPModel(gpytorch.models.ApproximateGP):\n    def __init__(self, inducing_points):\n        # Define the variational distribution\n        variational_distribution = gpytorch.variational.CholeskyVariationalDistribution(\n            inducing_points.size(0)\n        )\n        \n        # Define the variational strategy\n        variational_strategy = gpytorch.variational.VariationalStrategy(\n            self, inducing_points, variational_distribution,\n            learn_inducing_locations=True\n        )\n        \n        super(VariationalGPModel, self).__init__(variational_strategy)\n        \n        # Mean and covariance modules\n        self.mean_module = gpytorch.means.ConstantMean()\n        self.covar_module = gpytorch.kernels.ScaleKernel(\n            gpytorch.kernels.RBFKernel(ard_num_dims=inducing_points.size(1))\n        )\n\n    def forward(self, x):\n        mean_x = self.mean_module(x)\n        covar_x = self.covar_module(x)\n        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n\ndef prepare_data(data):\n    # Convert date to numerical features\n    data['Date'] = pd.to_datetime(data['Date'])\n    data['hour'] = data['Date'].dt.hour\n    data['day'] = data['Date'].dt.day\n    data['month'] = data['Date'].dt.month\n    \n    # Prepare feature matrix X\n    feature_cols = ['Factor_A', 'Factor_B', 'Factor_C', 'Factor_D', \n                   'Factor_E', 'Factor_F', 'hour', 'day', 'month']\n    X = data[feature_cols].values\n    y = data['Electric_Consumption'].values\n    \n    # Scale features\n    scaler_X = StandardScaler()\n    scaler_y = StandardScaler()\n    \n    X_scaled = scaler_X.fit_transform(X)\n    y_scaled = scaler_y.fit_transform(y.reshape(-1, 1)).flatten()\n    \n    return (torch.FloatTensor(X_scaled), \n            torch.FloatTensor(y_scaled), \n            scaler_X, \n            scaler_y)\n\ndef select_inducing_points(X_train, num_inducing=100):\n    \"\"\"Select inducing points using kmeans clustering\"\"\"\n    from sklearn.cluster import KMeans\n    \n    if len(X_train) <= num_inducing:\n        return X_train\n    \n    kmeans = KMeans(n_clusters=num_inducing, random_state=42)\n    kmeans.fit(X_train)\n    inducing_points = torch.FloatTensor(kmeans.cluster_centers_)\n    \n    return inducing_points\n\ndef train_model(X_train, y_train, num_inducing=100, training_iterations=100, batch_size=1024):\n    # Select inducing points\n    inducing_points = select_inducing_points(X_train, num_inducing)\n    \n    # Initialize model and likelihood\n    model = VariationalGPModel(inducing_points)\n    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n    \n    # Use the adam optimizer\n    optimizer = torch.optim.Adam([\n        {'params': model.parameters()},\n        {'params': likelihood.parameters()},\n    ], lr=0.01)\n\n    # Our loss object. We're using the VariationalELBO\n    mll = gpytorch.mlls.VariationalELBO(likelihood, model, num_data=y_train.size(0))\n    \n    # Create DataLoader for mini-batching\n    dataset = torch.utils.data.TensorDataset(X_train, y_train)\n    loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n    \n    model.train()\n    likelihood.train()\n    \n    for i in range(training_iterations):\n        epoch_loss = 0\n        for x_batch, y_batch in loader:\n            optimizer.zero_grad()\n            output = model(x_batch)\n            loss = -mll(output, y_batch)\n            loss.backward()\n            optimizer.step()\n            epoch_loss += loss.item()\n            \n        if (i + 1) % 10 == 0:\n            print(f'Iteration {i+1}/{training_iterations} - Loss: {epoch_loss:.3f}')\n    \n    return model, likelihood\n\ndef make_predictions(model, likelihood, X_test, batch_size=1024):\n    model.eval()\n    likelihood.eval()\n    \n    # Initialize predictions\n    means = []\n    lower = []\n    upper = []\n    \n    # Make predictions in batches\n    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n        for i in range(0, len(X_test), batch_size):\n            x_batch = X_test[i:i+batch_size]\n            # Get output from model\n            output = likelihood(model(x_batch))\n            # Get mean and confidence region for batch\n            means.append(output.mean)\n            lower_batch, upper_batch = output.confidence_region()\n            lower.append(lower_batch)\n            upper.append(upper_batch)\n    \n    # Concatenate results\n    mean_pred = torch.cat(means).numpy()\n    lower_pred = torch.cat(lower).numpy()\n    upper_pred = torch.cat(upper).numpy()\n    \n    return mean_pred, (lower_pred, upper_pred)\n\n# Assuming 'data' is your DataFrame\nX_train, y_train, scaler_X, scaler_y = prepare_data(df)\n\n# Train the model\nmodel, likelihood = train_model(X_train, y_train)\n\n# Make predictions\nmean_pred, (lower, upper) = make_predictions(model, likelihood, X_train)\n\n# Transform predictions back to original scale\nmean_pred = scaler_y.inverse_transform(mean_pred.reshape(-1, 1))\nlower = scaler_y.inverse_transform(lower.reshape(-1, 1))\nupper = scaler_y.inverse_transform(upper.reshape(-1, 1))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-28T21:05:13.658315Z","iopub.execute_input":"2024-12-28T21:05:13.658706Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Iteration 10/100 - Loss: 11.957\nIteration 20/100 - Loss: 5.787\n","output_type":"stream"}],"execution_count":null}]}