{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":76728,"databundleVersionId":9057646,"sourceType":"competition"}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-13T15:27:48.869321Z","iopub.execute_input":"2024-10-13T15:27:48.869778Z","iopub.status.idle":"2024-10-13T15:27:48.877763Z","shell.execute_reply.started":"2024-10-13T15:27:48.869722Z","shell.execute_reply":"2024-10-13T15:27:48.876801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install pymc aesara\n","metadata":{"execution":{"iopub.status.busy":"2024-10-13T15:47:34.525857Z","iopub.execute_input":"2024-10-13T15:47:34.526206Z","iopub.status.idle":"2024-10-13T15:47:47.587828Z","shell.execute_reply.started":"2024-10-13T15:47:34.526176Z","shell.execute_reply":"2024-10-13T15:47:47.586698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pymc as pm\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport arviz as az\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler, RobustScaler\nfrom sklearn.model_selection import train_test_split\n\ndf=pd.read_csv('/kaggle/input/playground-series-s4e9/train.csv')\n\n\ndef preprocess_data(df):\n    df = df.copy()\n    \n    df['log_price'] = np.log(df['price'])\n    df['engine_size'] = df['engine'].str.extract('(\\d+\\.\\d+|\\d+)').astype(float)\n    df['is_hybrid_electric'] = df['fuel_type'].str.contains('Hybrid|Electric', case=False, na=False).astype(int)\n\n    categorical_cols = ['brand', 'model', 'fuel_type', 'transmission', 'ext_col', 'int_col', 'accident', 'clean_title']\n    for col in categorical_cols:\n        df[col] = LabelEncoder().fit_transform(df[col].fillna('Unknown'))\n    \n    numerical_cols = ['model_year', 'milage', 'engine_size']\n    df[numerical_cols] = RobustScaler().fit_transform(df[numerical_cols])\n    \n    df = df.drop(['engine','price'], axis=1)\n\n    return df[:50000]\n\ndf = preprocess_data(df)\n\nprint(df.isna().sum())\n\ndf = df.dropna()\n\n# Split the data\nX = df.drop('log_price', axis=1)\ny = df['log_price']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Print some statistics about the target variable\nprint(\"Target variable statistics:\")\nprint(y_train.describe())","metadata":{"execution":{"iopub.status.busy":"2024-10-13T15:54:23.127028Z","iopub.execute_input":"2024-10-13T15:54:23.127421Z","iopub.status.idle":"2024-10-13T15:54:25.795759Z","shell.execute_reply.started":"2024-10-13T15:54:23.127381Z","shell.execute_reply":"2024-10-13T15:54:25.794779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create PyMC model\nwith pm.Model() as model:\n    # Priors for unknown model parameters\n    intercept = pm.Normal(\"intercept\", mu=0, sigma=1)\n    beta = pm.Normal(\"beta\", mu=0, sigma=1, shape=X_train.shape[1])\n    \n    # Expected value of outcome\n    mu = intercept + pm.math.dot(X_train, beta)\n    \n    # Likelihood (sampling distribution) of observations\n    sigma = pm.HalfNormal(\"sigma\", sigma=1)\n    y_obs = pm.Normal(\"y_obs\", mu=mu, sigma=sigma, observed=y_train)\n\n# Perform inference\nwith model:\n    trace = pm.sample(2000, tune=1000, return_inferencedata=True)\n\n# Analyze results\naz.plot_trace(trace)\nplt.show()\n\nsummary = az.summary(trace)\nprint(summary)","metadata":{"execution":{"iopub.status.busy":"2024-10-13T15:54:25.797600Z","iopub.execute_input":"2024-10-13T15:54:25.798395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to predict prices\ndef predict_price(X, trace):\n    intercept = trace.posterior['intercept'].mean(dim=['chain', 'draw']).values\n    beta = trace.posterior['beta'].mean(dim=['chain', 'draw']).values\n    return intercept + np.dot(X, beta)\n\n\n# Predict prices for test set\ntest_predictions = predict_price(X_test, trace)","metadata":{"execution":{"iopub.status.busy":"2024-10-13T15:54:02.623685Z","iopub.execute_input":"2024-10-13T15:54:02.624044Z","iopub.status.idle":"2024-10-13T15:54:02.639996Z","shell.execute_reply.started":"2024-10-13T15:54:02.624006Z","shell.execute_reply":"2024-10-13T15:54:02.638635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# y_test = np.exp(y_test)\n\n# Calculate Mean Absolute Error\nmae = np.mean(np.abs(test_predictions - y_test))\nprint(f\"Mean Absolute Error: ${mae:.2f}\")\n\n# Plot predicted vs actual prices\nplt.figure(figsize=(10, 6))\nplt.scatter(y_test, test_predictions, alpha=0.5)\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\nplt.xlabel(\"Actual Price\")\nplt.ylabel(\"Predicted Price\")\nplt.title(\"Predicted vs Actual Car Prices\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-13T15:54:02.642177Z","iopub.execute_input":"2024-10-13T15:54:02.642991Z","iopub.status.idle":"2024-10-13T15:54:02.955611Z","shell.execute_reply.started":"2024-10-13T15:54:02.642943Z","shell.execute_reply":"2024-10-13T15:54:02.954651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Predict prices for test set\ny_pred = predict_price(X_test.values, trace)\n\n# Calculate RMSE\nrmse = np.sqrt(np.mean((y_test - y_pred)**2))\nprint(f\"RMSE: {rmse}\")\n\n# Print a few predictions alongside actual prices\nprint(\"\\nSample predictions:\")\nfor i in range(5):\n    print(f\"Actual: ${y_test.iloc[i]:.2f}, Predicted: ${y_pred[i]:.2f}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-13T15:54:10.487023Z","iopub.execute_input":"2024-10-13T15:54:10.487674Z","iopub.status.idle":"2024-10-13T15:54:10.501853Z","shell.execute_reply.started":"2024-10-13T15:54:10.487634Z","shell.execute_reply":"2024-10-13T15:54:10.500341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Feature importance\nfeature_importance = {col: np.abs(trace.posterior['beta'].mean(dim=['chain', 'draw']).values)[idx] for idx, col in enumerate(X.columns)}\nsorted_features = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)\n\nplt.figure(figsize=(12, 6))\nplt.bar([x[0] for x in sorted_features], [x[1] for x in sorted_features])\nplt.xticks(rotation=90)\nplt.xlabel(\"Features\")\nplt.ylabel(\"Absolute Coefficient Value\")\nplt.title(\"Feature Importance\")\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-13T15:28:54.853944Z","iopub.execute_input":"2024-10-13T15:28:54.855086Z","iopub.status.idle":"2024-10-13T15:28:56.167806Z","shell.execute_reply.started":"2024-10-13T15:28:54.855025Z","shell.execute_reply":"2024-10-13T15:28:56.166724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install GPy GPyOpt","metadata":{"execution":{"iopub.status.busy":"2024-10-13T15:28:56.168987Z","iopub.execute_input":"2024-10-13T15:28:56.169309Z","iopub.status.idle":"2024-10-13T15:29:07.629752Z","shell.execute_reply.started":"2024-10-13T15:28:56.169274Z","shell.execute_reply":"2024-10-13T15:29:07.628736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Bayesian Optimization with Gaussian Process\nimport GPy\nimport GPyOpt\n\n# Define the objective function (negative predicted price)\ndef objective_function(X):\n    return -predict_price(X, trace)\n\n# Define the domain\nbounds = []\nfor col in X.columns:\n    bounds.append({'name': col, 'type': 'continuous', 'domain': (X[col].min(), X[col].max())})\n\n# Create the Bayesian optimization object\noptimizer = GPyOpt.methods.BayesianOptimization(f=objective_function, \n                                                domain=bounds,\n                                                model_type='GP',\n                                                acquisition_type='EI',\n                                                normalize_Y=True)\n\n# Run the optimization\noptimizer.run_optimization(max_iter=50)\n\n# Get the best solution\nbest_x = optimizer.x_opt\nbest_y = -optimizer.fx_opt\n\nprint(\"Best configuration:\")\nfor i, col in enumerate(X.columns):\n    print(f\"{col}: {best_x[i]}\")\nprint(f\"Predicted price: ${best_y:.2f}\")\n\n# Plot the optimization results\noptimizer.plot_acquisition()\nplt.show()\n\noptimizer.plot_convergence()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-13T15:29:07.631371Z","iopub.execute_input":"2024-10-13T15:29:07.631722Z","iopub.status.idle":"2024-10-13T15:29:37.384773Z","shell.execute_reply.started":"2024-10-13T15:29:07.631687Z","shell.execute_reply":"2024-10-13T15:29:37.383832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Submission","metadata":{}},{"cell_type":"code","source":"from sklearn.impute import SimpleImputer\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nfrom sklearn.ensemble import RandomForestRegressor\n\ndef preprocess_data_test(df):\n    df = df.copy()\n    \n    # Extract numeric engine size or horsepower\n    df['engine_size'] = df['engine'].str.extract('(\\d+\\.\\d+|\\d+)').astype(float)\n    df['engine_size'].interpolate(method='linear', inplace=True)\n    \n    # Create a binary column for hybrid/electric vehicles\n    df['is_hybrid_electric'] = df['fuel_type'].str.contains('Hybrid|Electric', case=False, na=False).astype(int)\n    \n    # Inpute Missing values\n    df['fuel_type'].fillna(df['fuel_type'].mode()[0])\n    \n    # 2. Accident\n    # For accident, we might assume that missing values indicate no accident\n    df['accident'] = df['accident'].fillna('None reported')\n\n    # 3. Clean Title\n    # For clean title, we might use a similar approach as accident\n    df['clean_title'] = df['clean_title'].fillna('Yes')\n    \n    # Example using Iterative Imputation (MICE)\n#     numerical_columns = ['model_year', 'milage']  # Add any other numerical columns\n#     categorical_columns = ['brand', 'model', 'fuel_type', 'engine', 'transmission', 'ext_col', 'int_col', 'accident', 'clean_title']\n\n#     # One-hot encode categorical variables\n#     df_encoded = pd.get_dummies(df, columns=categorical_columns)\n\n#     # Create the IterativeImputer\n#     iterative_imputer = IterativeImputer(estimator=RandomForestRegressor(), max_iter=10, random_state=0)\n\n#     # Fit and transform the data\n#     df_imputed = pd.DataFrame(iterative_imputer.fit_transform(df_encoded), columns=df_encoded.columns)\n\n    \n    # Encode categorical variables\n    categorical_cols = ['brand', 'model', 'fuel_type', 'transmission', 'ext_col', 'int_col', 'accident', 'clean_title']\n    for col in categorical_cols:\n        df[col] = LabelEncoder().fit_transform(df[col].fillna('Unknown'))\n    \n    # Scale numerical features\n    numerical_cols = ['model_year', 'milage', 'engine_size']\n    df[numerical_cols] = RobustScaler().fit_transform(df[numerical_cols])\n    \n    df = df.drop(['engine'], axis=1)\n\n    return df\n\ntest_df = pd.read_csv('/kaggle/input/playground-series-s4e9/test.csv')\ntest_df = preprocess_data_test(test_df)\ny_pred = pd.DataFrame({\n    'id': test_df['id'],\n    'price': np.exp(predict_price(test_df, trace))\n})\ny_pred.to_csv('submission.csv', index=False)\ny_pred.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2024-10-13T15:29:37.386656Z","iopub.execute_input":"2024-10-13T15:29:37.387074Z","iopub.status.idle":"2024-10-13T15:29:39.005338Z","shell.execute_reply.started":"2024-10-13T15:29:37.387028Z","shell.execute_reply":"2024-10-13T15:29:39.004425Z"},"trusted":true},"execution_count":null,"outputs":[]}]}