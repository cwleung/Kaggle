{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "# we need to install HuggingFace datasets library\n",
    "!pip install datasets"
   ],
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "execution": {
     "iopub.status.busy": "2022-05-03T19:34:27.391815Z",
     "iopub.execute_input": "2022-05-03T19:34:27.394095Z",
     "iopub.status.idle": "2022-05-03T19:34:43.396716Z",
     "shell.execute_reply.started": "2022-05-03T19:34:27.393853Z",
     "shell.execute_reply": "2022-05-03T19:34:43.395470Z"
    },
    "trusted": true
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# to disable wandb logging\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "import torch\n",
    "\n",
    "# Transformers\n",
    "# installed with pip command above\n",
    "from datasets import Dataset\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-05-03T19:34:43.399475Z",
     "iopub.execute_input": "2022-05-03T19:34:43.400253Z",
     "iopub.status.idle": "2022-05-03T19:34:51.568347Z",
     "shell.execute_reply.started": "2022-05-03T19:34:43.400203Z",
     "shell.execute_reply": "2022-05-03T19:34:51.567182Z"
    },
    "trusted": true
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### reading the entire training dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "TRAIN_FILE = \"/kaggle/input/nlp-getting-started/train.csv\"\n",
    "\n",
    "orig_train = pd.read_csv(TRAIN_FILE)\n",
    "\n",
    "# num of distinct labels in target\n",
    "NUM_LABELS = orig_train['target'].nunique()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-05-03T19:34:51.571565Z",
     "iopub.execute_input": "2022-05-03T19:34:51.571848Z",
     "iopub.status.idle": "2022-05-03T19:34:51.630408Z",
     "shell.execute_reply.started": "2022-05-03T19:34:51.571818Z",
     "shell.execute_reply": "2022-05-03T19:34:51.629571Z"
    },
    "trusted": true
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### split in train/validation set and keep only useful columns"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# do the train/validation split\n",
    "\n",
    "SEED = 1432\n",
    "VALID_FRAC = 0.2\n",
    "USED_COLUMNS = ['text', 'target']\n",
    "\n",
    "\n",
    "train_df, valid_df = train_test_split(orig_train, test_size=VALID_FRAC, random_state=42)\n",
    "\n",
    "train_df = train_df[USED_COLUMNS]\n",
    "valid_df = valid_df[USED_COLUMNS]\n",
    "\n",
    "print(f\"There are {train_df.shape[0]} samples in train set\")\n",
    "print(f\"There are {valid_df.shape[0]} samples in valid set\")\n",
    "\n",
    "# rename rating to label\n",
    "train_df = train_df.rename(columns={\"target\": \"label\"})\n",
    "valid_df = valid_df.rename(columns={\"target\": \"label\"})"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-05-03T19:34:51.633151Z",
     "iopub.execute_input": "2022-05-03T19:34:51.633432Z",
     "iopub.status.idle": "2022-05-03T19:34:51.651607Z",
     "shell.execute_reply.started": "2022-05-03T19:34:51.633394Z",
     "shell.execute_reply": "2022-05-03T19:34:51.650047Z"
    },
    "trusted": true
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### create HF datasets\n",
    "\n",
    "to prepare data fro the training we need a train and a validation dataset where text has been transformed in token and encoded"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# start building the dataset objects expected from transformers\n",
    "ds_train = Dataset.from_pandas(train_df.reset_index(drop=True))\n",
    "ds_valid = Dataset.from_pandas(valid_df.reset_index(drop=True))"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-05-03T19:34:51.653617Z",
     "iopub.execute_input": "2022-05-03T19:34:51.654211Z",
     "iopub.status.idle": "2022-05-03T19:34:51.684146Z",
     "shell.execute_reply.started": "2022-05-03T19:34:51.654165Z",
     "shell.execute_reply": "2022-05-03T19:34:51.683182Z"
    },
    "trusted": true
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "ds_train.features"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-05-03T19:34:51.685451Z",
     "iopub.execute_input": "2022-05-03T19:34:51.687869Z",
     "iopub.status.idle": "2022-05-03T19:34:51.698271Z",
     "shell.execute_reply.started": "2022-05-03T19:34:51.687823Z",
     "shell.execute_reply": "2022-05-03T19:34:51.696694Z"
    },
    "trusted": true
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Tokenization"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# here we define the pre-trained transformer we are using. In this NB we will be using roberta-large and corresponding tokenizer\n",
    "MODEL_CKPT = \"roberta-large\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_CKPT)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-05-03T19:34:51.700327Z",
     "iopub.execute_input": "2022-05-03T19:34:51.700738Z",
     "iopub.status.idle": "2022-05-03T19:34:59.576822Z",
     "shell.execute_reply.started": "2022-05-03T19:34:51.700694Z",
     "shell.execute_reply": "2022-05-03T19:34:59.575925Z"
    },
    "trusted": true
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# this function will be applied to both set for tokenization to add columns with token encoded\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=True, truncation=True, max_length=50)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-05-03T19:34:59.578344Z",
     "iopub.execute_input": "2022-05-03T19:34:59.578618Z",
     "iopub.status.idle": "2022-05-03T19:34:59.583119Z",
     "shell.execute_reply.started": "2022-05-03T19:34:59.578578Z",
     "shell.execute_reply": "2022-05-03T19:34:59.582174Z"
    },
    "trusted": true
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# and here we have the final HF datasets\n",
    "ds_train_encoded = ds_train.map(tokenize, batched=True, batch_size=None)\n",
    "ds_valid_encoded = ds_valid.map(tokenize, batched=True, batch_size=None)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-05-03T19:34:59.584898Z",
     "iopub.execute_input": "2022-05-03T19:34:59.585478Z",
     "iopub.status.idle": "2022-05-03T19:35:00.954186Z",
     "shell.execute_reply.started": "2022-05-03T19:34:59.585438Z",
     "shell.execute_reply": "2022-05-03T19:35:00.953188Z"
    },
    "trusted": true
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# have a look\n",
    "ds_train_encoded"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-05-03T19:35:00.958060Z",
     "iopub.execute_input": "2022-05-03T19:35:00.958694Z",
     "iopub.status.idle": "2022-05-03T19:35:00.966365Z",
     "shell.execute_reply.started": "2022-05-03T19:35:00.958649Z",
     "shell.execute_reply": "2022-05-03T19:35:00.965180Z"
    },
    "trusted": true
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "as we can see we have two columns added: input_ids and attention mask, that will be used during the training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# prepare the training on GPU (if available)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = (AutoModelForSequenceClassification.from_pretrained(MODEL_CKPT, num_labels=NUM_LABELS).to(device))"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-05-03T19:35:00.969058Z",
     "iopub.execute_input": "2022-05-03T19:35:00.969623Z",
     "iopub.status.idle": "2022-05-03T19:36:17.085503Z",
     "shell.execute_reply.started": "2022-05-03T19:35:00.969563Z",
     "shell.execute_reply": "2022-05-03T19:36:17.084169Z"
    },
    "trusted": true
   },
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# this function is used to compute the metrics (accuracy, f1-score) that will be computed during validation phases\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    \n",
    "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    \n",
    "    return {\"accuracy\": acc, \"f1\": f1}"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-05-03T19:36:17.087035Z",
     "iopub.execute_input": "2022-05-03T19:36:17.087584Z",
     "iopub.status.idle": "2022-05-03T19:36:17.097471Z",
     "shell.execute_reply.started": "2022-05-03T19:36:17.087523Z",
     "shell.execute_reply": "2022-05-03T19:36:17.096158Z"
    },
    "trusted": true
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# params for training\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# be careful, with 5 epochs local storage will be filled and you run out of space\n",
    "EPOCHS = 3\n",
    "LR = 1e-5\n",
    "W_DECAY = 0.01\n",
    "\n",
    "# to disable wandb logging ---> best, see report_to\n",
    "# os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "logging_steps = len(ds_train_encoded) // BATCH_SIZE\n",
    "\n",
    "model_name = f\"{MODEL_CKPT}-finetuned-tweets\"\n",
    "\n",
    "training_args = TrainingArguments(output_dir=model_name,\n",
    "                                  num_train_epochs=EPOCHS,\n",
    "                                  # changed\n",
    "                                  learning_rate=LR,\n",
    "                                  per_device_train_batch_size=BATCH_SIZE,\n",
    "                                  per_device_eval_batch_size=BATCH_SIZE,\n",
    "                                  weight_decay=W_DECAY,\n",
    "                                  evaluation_strategy=\"epoch\",\n",
    "                                  save_strategy=\"epoch\",\n",
    "                                  disable_tqdm=False,\n",
    "                                  logging_steps=logging_steps,\n",
    "                                  push_to_hub=False, \n",
    "                                  log_level=\"error\",\n",
    "                                  load_best_model_at_end=True,\n",
    "                                  # to disable wandb logging\n",
    "                                  report_to=\"none\"\n",
    "                                 )"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-05-03T19:36:17.101807Z",
     "iopub.execute_input": "2022-05-03T19:36:17.102253Z",
     "iopub.status.idle": "2022-05-03T19:36:17.123287Z",
     "shell.execute_reply.started": "2022-05-03T19:36:17.102190Z",
     "shell.execute_reply": "2022-05-03T19:36:17.122052Z"
    },
    "trusted": true
   },
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#\n",
    "# and here we will do the training\n",
    "#\n",
    "trainer = Trainer(model=model, args=training_args, \n",
    "                  compute_metrics=compute_metrics,\n",
    "                  train_dataset=ds_train_encoded,\n",
    "                  eval_dataset=ds_valid_encoded,\n",
    "                  tokenizer=tokenizer)\n",
    "trainer.train();"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-05-03T19:36:17.127381Z",
     "iopub.execute_input": "2022-05-03T19:36:17.128380Z",
     "iopub.status.idle": "2022-05-03T19:43:10.681002Z",
     "shell.execute_reply.started": "2022-05-03T19:36:17.128321Z",
     "shell.execute_reply": "2022-05-03T19:43:10.679861Z"
    },
    "trusted": true
   },
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# compute f1 score on the best model (chosen on valid_loss)\n",
    "f1 = trainer.predict(ds_valid_encoded).metrics['test_f1']\n",
    "    \n",
    "print(f\"F1 score is: {round(f1, 4)}\")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-05-03T19:43:10.682511Z",
     "iopub.execute_input": "2022-05-03T19:43:10.683662Z",
     "iopub.status.idle": "2022-05-03T19:43:18.909324Z",
     "shell.execute_reply.started": "2022-05-03T19:43:10.683591Z",
     "shell.execute_reply": "2022-05-03T19:43:18.908388Z"
    },
    "trusted": true
   },
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### final remarks:\n",
    "* as you can see, the score is good: F1 = 0.84; Obviously here it depends on the train/valid split done\n",
    "* a better result could be obtained, for example, using k-fold split; The final result is a set of k models and you need to avg predictions\n",
    "\n",
    "I have not put here the code to do predictions on the test set. It is not difficult. Only one thing: you need to do it in batches (to avoid OOM on GPU). It is left as an exercise."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}
